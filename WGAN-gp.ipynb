{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-gp demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains methods from paper: Improved Training of Wasserstein GANs (https://arxiv.org/abs/1704.00028), which introduces gradient penalty loss and layer normalization in the discriminator network, and improves the training stability and performance of GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images_train, label), _ = mnist.load_data()\n",
    "images_train = images_train / 255.0\n",
    "images_train = images_train[:,:,:,np.newaxis]\n",
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "def generator(latent_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input([latent_dim]),\n",
    "        tf.keras.layers.Dense(units=(7 * 7 * 256), use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Reshape((7, 7, 256)),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=128,\n",
    "            kernel_size=5,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=64,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "            activation='sigmoid',\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input([28, 28, 1]),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=2,\n",
    "                               padding='same'),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        \n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=5, strides=2,\n",
    "                               padding='same'),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        \n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, generator, discriminator, latent, gp_weight=10, disc_steps=3):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.gp_weight = gp_weight\n",
    "        self.disc_steps = disc_steps\n",
    "        self.latent = latent\n",
    "        \n",
    "    def gradient_penalty(self, real, fake):\n",
    "        batch_size = tf.shape(real)[0]\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake - real\n",
    "        interpolated = real + alpha * diff\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    @staticmethod\n",
    "    def discriminator_loss(real_logits, fake_logits):\n",
    "        real_loss = tf.reduce_mean(real_logits)\n",
    "        fake_loss = tf.reduce_mean(fake_logits)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def generator_loss(fake_logits):\n",
    "        return -tf.reduce_mean(fake_logits)\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        real = inputs\n",
    "        \n",
    "        disc_vars = self.discriminator.trainable_variables\n",
    "        gen_vars = self.generator.trainable_variables\n",
    "\n",
    "        for i in range(self.disc_steps): \n",
    "            noise = tf.random.normal(shape=[tf.shape(real)[0], self.latent])\n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                fake = self.generator(noise, training=True)\n",
    "                real_logits = self.discriminator(real, training=True)\n",
    "                fake_logits = self.discriminator(fake, training=True)\n",
    "                disc_loss = self.discriminator_loss(real_logits, fake_logits)\n",
    "                disc_loss += self.gp_weight * self.gradient_penalty(real, fake)\n",
    "\n",
    "            disc_grads = disc_tape.gradient(disc_loss, disc_vars)\n",
    "            self.optimizer.apply_gradients(zip(disc_grads, disc_vars))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            noise = tf.random.normal(shape=[tf.shape(real)[0], self.latent])\n",
    "            fake = self.generator(noise, training=True)\n",
    "            real_logits = self.discriminator(real, training=True)\n",
    "            fake_logits = self.discriminator(fake, training=True)\n",
    "            gen_loss = self.generator_loss(fake_logits)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, gen_vars)\n",
    "        self.optimizer.apply_gradients(zip(gen_grads, gen_vars))\n",
    "\n",
    "        return {'gen_loss': gen_loss, 'disc_loss': disc_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "epochs = 20\n",
    "lr = 2e-4\n",
    "batch_size = 256\n",
    "model = GAN(generator=generator(latent_dim), \n",
    "            discriminator=discriminator(),\n",
    "            latent = latent_dim)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.5, beta_2=0.9, learning_rate=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(images_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display results\n",
    "def img_grid(imgs, rows, cols, title):\n",
    "    fig = plt.figure()\n",
    "    for i, j in np.ndindex(rows, cols):\n",
    "        plt.subplot(rows, cols, i*cols+j+1)\n",
    "        plt.imshow(np.repeat(imgs[i*cols+j], 3, -1))\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "cols = 8\n",
    "rows = 8\n",
    "inp = images_train[:cols*rows]\n",
    "gen = model.generator.predict(np.random.normal(size=(cols*rows, latent_dim)))\n",
    "img_grid(inp, rows, cols, 'Original')\n",
    "img_grid(gen, rows, cols, 'Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
