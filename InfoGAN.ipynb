{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cGAN and cVAE both rely on supervised training to have control over features (defined by labels) of generated data. InfoGAN, on the other hand, does not require labels and instead extracts latent features using an autoencoder-like architecture called auxiliary model. This allows InfoGAN to disentangle latent space without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images_train, label), (images_test, label_test) = mnist.load_data()\n",
    "images_train = images_train[...,np.newaxis] / 255.0\n",
    "images_test = images_test[...,np.newaxis] / 255.0\n",
    "images_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input([latent_dim]),\n",
    "        tf.keras.layers.Dense(units=(7 * 7 * 256), use_bias=False),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Reshape((7, 7, 256)),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=128,\n",
    "            kernel_size=5,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=64,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Conv2DTranspose(\n",
    "            filters=1,\n",
    "            kernel_size=5,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            use_bias=False,\n",
    "            activation='sigmoid',\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "\n",
    "def discriminator_auxiliary(conditional_dim, categorical_dim):\n",
    "    inp = tf.keras.layers.Input([28, 28, 1])\n",
    "    \n",
    "    # shared\n",
    "    x = tf.keras.layers.Conv2D(filters=32, \n",
    "                               kernel_size=5, \n",
    "                               strides=2,\n",
    "                               padding='same')(inp)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)    \n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=64, \n",
    "                               kernel_size=5, \n",
    "                               strides=2,\n",
    "                               padding='same')(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    # discriminator \n",
    "    d = tf.keras.layers.Conv2D(filters=128,\n",
    "                               kernel_size=7, \n",
    "                               strides=1, \n",
    "                               padding='valid')(x)\n",
    "    d = tf.keras.layers.LayerNormalization()(d)\n",
    "    d = tf.keras.layers.LeakyReLU()(d)\n",
    "\n",
    "    d = tf.keras.layers.Flatten()(d)\n",
    "    d = tf.keras.layers.Dense(64)(d)\n",
    "    d = tf.keras.layers.Dense(1)(d)\n",
    "    \n",
    "    disc = tf.keras.Model(inp, d)\n",
    "\n",
    "    # auxiliary   \n",
    "    q = tf.keras.layers.Conv2D(filters=128,\n",
    "                               kernel_size=7, \n",
    "                               strides=1, \n",
    "                               padding='valid')(x)\n",
    "    q = tf.keras.layers.LayerNormalization()(q)\n",
    "    q = tf.keras.layers.LeakyReLU()(q)\n",
    "    \n",
    "    q = tf.keras.layers.Flatten()(q)\n",
    "    q = tf.keras.layers.Dense(64)(q)\n",
    "    q = tf.keras.layers.Dense(conditional_dim)(q)\n",
    "\n",
    "    softmax = tf.keras.activations.softmax(q[:, :categorical_dim], -1)\n",
    "    q = tf.concat([softmax, q[:, categorical_dim:]], -1)\n",
    "    \n",
    "    auxiliary = tf.keras.Model(inp, q)\n",
    "    \n",
    "    return disc, auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_dim, \n",
    "                 conditional_dim, \n",
    "                 categorical_dim, \n",
    "                 gp_weight=10, \n",
    "                 disc_steps=3):\n",
    "        super().__init__()\n",
    "        self.gp_weight = gp_weight\n",
    "        self.disc_steps = disc_steps\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.conditional_dim = conditional_dim\n",
    "        self.categorical_dim = categorical_dim\n",
    "                \n",
    "        self.discriminator, self.auxiliary = discriminator_auxiliary(self.conditional_dim, \n",
    "                                                                     self.categorical_dim)\n",
    "        self.generator = generator(self.latent_dim)\n",
    "        \n",
    "    def gradient_penalty(self, real, fake):\n",
    "        batch_size = tf.shape(real)[0]\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake - real\n",
    "        interpolated = real + alpha * diff\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    @staticmethod\n",
    "    def discriminator_loss(real_logits, fake_logits):\n",
    "        real_loss = tf.reduce_mean(real_logits)\n",
    "        fake_loss = tf.reduce_mean(fake_logits)\n",
    "        return fake_loss - real_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def cat_loss(c_true, c_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(c_true, c_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def l2_loss(c_true, c_pred):    \n",
    "        return tf.reduce_mean((c_true-c_pred)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator_loss(fake_logits):\n",
    "        return -tf.reduce_mean(fake_logits)\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        real = inputs\n",
    "        \n",
    "        disc_vars = self.discriminator.trainable_variables\n",
    "        gen_vars = self.generator.trainable_variables \n",
    "        aux_vars = self.auxiliary.trainable_variables + self.generator.trainable_variables \n",
    "\n",
    "        for i in range(self.disc_steps): \n",
    "            random_category = tf.random.uniform(shape=(tf.shape(real)[0], ), minval=0, maxval=10, dtype=tf.int64)\n",
    "            random_onehot = tf.one_hot(random_category, self.categorical_dim)\n",
    "            \n",
    "            noise = tf.random.normal(shape=[tf.shape(real)[0], self.latent_dim-self.categorical_dim])\n",
    "            noise = tf.concat([random_onehot, noise], -1)\n",
    "            \n",
    "            with tf.GradientTape() as disc_tape:\n",
    "                fake = self.generator(noise, training=True)\n",
    "                real_logits = self.discriminator(real, training=True)\n",
    "                fake_logits = self.discriminator(fake, training=True)\n",
    "                disc_loss = self.discriminator_loss(real_logits, fake_logits)\n",
    "                disc_loss += self.gp_weight * self.gradient_penalty(real, fake)\n",
    "\n",
    "            disc_grads = disc_tape.gradient(disc_loss, disc_vars)\n",
    "            self.optimizer.apply_gradients(zip(disc_grads, disc_vars))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = self.generator(noise, training=True)\n",
    "            real_logits = self.discriminator(real, training=True)\n",
    "            fake_logits = self.discriminator(fake, training=True)\n",
    "            gen_loss = self.generator_loss(fake_logits)\n",
    "            \n",
    "        gen_grads = gen_tape.gradient(gen_loss, gen_vars)\n",
    "        self.optimizer.apply_gradients(zip(gen_grads, gen_vars))\n",
    "\n",
    "        with tf.GradientTape() as aux_tape:\n",
    "            fake = self.generator(noise, training=True)\n",
    "            condition = self.auxiliary(fake)\n",
    "            \n",
    "            true_condition = noise[:, :self.conditional_dim]\n",
    "            true_category = true_condition[:, :self.categorical_dim]\n",
    "            true_gaussian = true_condition[:, self.categorical_dim:]\n",
    "            \n",
    "            cat_loss = self.cat_loss(true_category, condition[:, :self.categorical_dim])\n",
    "            l2_loss = self.l2_loss(true_gaussian, condition[:, self.categorical_dim:])\n",
    "            auxiliary_loss = cat_loss + l2_loss\n",
    "            \n",
    "        aux_grads = aux_tape.gradient(auxiliary_loss, aux_vars)\n",
    "        self.optimizer.apply_gradients(zip(aux_grads, aux_vars))\n",
    "        \n",
    "        return {'gen_loss': gen_loss,\n",
    "                'disc_loss': disc_loss,\n",
    "                'auxiliary_loss':auxiliary_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 62\n",
    "categorical_dim = 10 \n",
    "gaussian_dim = 2\n",
    "conditional_dim = categorical_dim + gaussian_dim\n",
    "epochs = 2000\n",
    "lr = 1e-3\n",
    "batch_size = 256\n",
    "\n",
    "model = GAN(latent_dim = latent_dim,\n",
    "            categorical_dim = categorical_dim,\n",
    "            conditional_dim = conditional_dim)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(beta_1=0.5, beta_2=0.9, learning_rate=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='gen_loss', \n",
    "                                                           min_delta=0, \n",
    "                                                           patience=20, \n",
    "                                                           restore_best_weights=True)      \n",
    "model.fit(images_train, \n",
    "          epochs=50, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display results\n",
    "def img_grid(imgs, rows, cols, title, epoch=0):\n",
    "    fig = plt.figure()\n",
    "    for i, j in np.ndindex(rows, cols):\n",
    "        plt.subplot(rows, cols, i*cols+j+1)\n",
    "        plt.imshow(np.repeat(imgs[i*cols+j], 3, -1))\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "cols = 15\n",
    "rows = 10\n",
    "\n",
    "r = np.random.normal(0, 0.2, size=(rows,cols,latent_dim)) # here we use small sigma to generate closer-to-'average' digits\n",
    "r[:,:, 11] =  np.linspace(-2, 2, cols)\n",
    "r[:, :, :10] = np.eye(10)[:, np.newaxis].repeat(cols, 1)\n",
    "r = r.reshape(-1, latent_dim) \n",
    "\n",
    "gen = model.generator.predict(r)\n",
    "img_grid(gen, rows, cols, 'Generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the auxiliary model itself is a classification model\n",
    "# lets take a look at its performance\n",
    "digit_order = np.array([5,4,1,3,8,0,9,6,2,7]) # digits read from the generated images above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =  model.auxiliary.predict(images_train, batch_size=512)\n",
    "pred_conf = out[:, :10]\n",
    "latent = out[:, 10:]\n",
    "pred_result = np.argmax(pred_conf, -1)\n",
    "conf_max = np.max(pred_conf, -1)\n",
    "conf_argsort = np.argsort(conf_max, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = digit_order[pred_result] == label\n",
    "\n",
    "# accuracy over increasing prediction confidence\n",
    "test_range = range(0, len(compare), 5000)\n",
    "compare_conf = [compare[c:].mean() for c in test_range]\n",
    "plt.plot(test_range, compare_conf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
