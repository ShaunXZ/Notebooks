{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE-transformer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation using VQ-VAE requires sampling an generative model (e.g. PixelCNN) in the discretized latent space. A recent work (https://arxiv.org/abs/2012.09841) replaced PixelCNN with transformer and showed impressive results. This notebook contains a simple implementation of the VQ-VAE-transformer model for conditional image generation with mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, MaxPool2D, MultiHeadAttention, BatchNormalization, Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"Train images:\", x_train.shape)\n",
    "print(\"Train labels:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/HenningBuhl/VQ-VAE_Keras_Implementation/blob/master/VQ_VAE_Keras_MNIST_Example.ipynb\n",
    "class VQVAELayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_embeddings=8, commitment_cost=0.25,\n",
    "                 initializer='uniform', epsilon=1e-10, name='vqvae', **kwargs):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.initializer = initializer\n",
    "        super(VQVAELayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg   \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # Add embedding weights.\n",
    "        self.w = self.add_weight(name='embedding',\n",
    "                                 shape=(self.embedding_dim, self.num_embeddings),\n",
    "                                 initializer=self.initializer,\n",
    "                                 trainable=True)\n",
    "\n",
    "        # Finalize building.\n",
    "        super(VQVAELayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Flatten input except for last dimension.\n",
    "        flat_inputs = tf.reshape(x, (-1, self.embedding_dim))\n",
    "\n",
    "        # Calculate distances of input to embedding vectors.\n",
    "        distances = (tf.reduce_sum(flat_inputs**2, axis=1, keepdims=True)\n",
    "                     - 2 * tf.matmul(flat_inputs, self.w)\n",
    "                     + tf.reduce_sum(self.w ** 2, axis=0, keepdims=True))\n",
    "\n",
    "        # Retrieve encoding indices.\n",
    "        encoding_indices = tf.argmax(-distances, axis=1)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        encoding_indices = tf.reshape(encoding_indices, tf.shape(x)[:-1])\n",
    "        quantized = self.quantize(encoding_indices)\n",
    "        \n",
    "        return quantized, encoding_indices\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        return self.w\n",
    "\n",
    "    def quantize(self, encoding_indices):\n",
    "        w = tf.transpose(self.embeddings.read_value())\n",
    "        return tf.nn.embedding_lookup(w, encoding_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.enc = Sequential([Conv2D(32, 3, padding='same', strides=2, activation='relu'),\n",
    "                               Conv2D(64, 3, padding='same',  strides=2, activation='relu'),\n",
    "                               Conv2D(self.embedding_dim, 3, padding='same', activation='relu')])\n",
    "        \n",
    "        self.dec = Sequential([Conv2DTranspose(64, 3, strides=(2, 2), padding='same', activation='relu'),\n",
    "                               Conv2DTranspose(32, 3, strides=(2, 2), padding='same', activation='relu'),\n",
    "                               Conv2DTranspose(1, 1, strides=(1, 1), activation='sigmoid', padding='same', name='output')])\n",
    "        \n",
    "        self.vq = VQVAELayer(embedding_dim, num_embeddings=self.vocabulary_size)\n",
    "        \n",
    "    def recons_loss(self, true, pred):\n",
    "        return tf.reduce_mean((true - pred)**2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_e = self.enc(inputs)\n",
    "        z_q, encoding_indices = self.vq(z_e)\n",
    "        x = self.dec(z_q)\n",
    "        return x, encoding_indices\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        train_vars = self.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_e = self.enc(inputs)\n",
    "            z_q, encoding_indices = self.vq(z_e)\n",
    "            straight_through = tf.keras.layers.Lambda(lambda x : x[0] + tf.stop_gradient(x[1] - x[0]))\n",
    "            \n",
    "            x = straight_through([z_e, z_q])\n",
    "            x = self.dec(x)\n",
    "            \n",
    "            vq_loss = tf.reduce_mean((tf.stop_gradient(z_e) - z_q)**2)\n",
    "            commit_loss = tf.reduce_mean((z_e - tf.stop_gradient(z_q))**2)\n",
    "\n",
    "            latent_loss = vq_loss + 0.25 * commit_loss\n",
    "            recons_loss = self.recons_loss(inputs, x)\n",
    "            \n",
    "            loss = recons_loss + latent_loss \n",
    "\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        \n",
    "        return {'recons_loss': recons_loss, 'latent_loss':latent_loss}\n",
    "    \n",
    "    def test_step(self, inputs):\n",
    "        \n",
    "        z_e = self.enc(inputs)\n",
    "        z_q, encoding_indices = self.vq(z_e)\n",
    "        x = self.dec(z_q)\n",
    "        \n",
    "        vq_loss = tf.reduce_mean((tf.stop_gradient(z_e) - z_q)**2)\n",
    "        commit_loss = tf.reduce_mean((z_e - tf.stop_gradient(z_q))**2)\n",
    "        latent_loss = vq_loss + 0.25 * commit_loss\n",
    "        recons_loss = self.recons_loss(inputs, x)\n",
    "\n",
    "        loss = recons_loss + latent_loss \n",
    "\n",
    "        return {'recons_loss': recons_loss, 'latent_loss':latent_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "vocabulary_size = 8\n",
    "model = VQVAE(embedding_dim, vocabulary_size)\n",
    "model.compile(optimizer='adam')\n",
    "model.build((None, x_train.shape[1], x_train.shape[2], 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_recons_loss', \n",
    "                                                           min_delta=0, \n",
    "                                                           patience=10,\n",
    "                                                           restore_best_weights=True)\n",
    "model.fit(x_train,\n",
    "          validation_data=(x_test, None),\n",
    "          batch_size=256, \n",
    "          epochs=60, \n",
    "          callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_grid(imgs, rows, cols, title):\n",
    "    fig = plt.figure()\n",
    "    for i, j in np.ndindex(rows, cols):\n",
    "        plt.subplot(rows, cols, i*rows+j+1)\n",
    "        plt.imshow(np.repeat(imgs[i*rows+j], 3, -1))\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "cols = 7\n",
    "rows = 7\n",
    "inp_x = x_test[:cols*rows]\n",
    "pred_x, ebd = model.predict(inp_x)\n",
    "img_grid(inp_x, rows, cols, 'Original')\n",
    "img_grid(pred_x, rows, cols, 'Reconstructed')\n",
    "img_grid(ebd[...,np.newaxis]/8, rows, cols, 'Reconstructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encoding_indices of training images\n",
    "batch = 256\n",
    "results = []\n",
    "for i in range(0, x_train.shape[0], batch):\n",
    "    x = x_train[i:i+batch]\n",
    "    out = model.predict(x)\n",
    "    results.append(out[1])\n",
    "results = np.concatenate(results, 0)\n",
    "\n",
    "train_seq = np.eye(8)[results.reshape(-1, 49)]\n",
    "train_seq_pad = np.zeros((60000,50,8))\n",
    "train_seq_pad[:, 1:] = train_seq\n",
    "\n",
    "train_label = np.eye(10)[y_train]\n",
    "train_label = np.repeat(train_label[:, np.newaxis], 50, 1)\n",
    "\n",
    "train_inp = np.concatenate([train_seq_pad, train_label], -1)\n",
    "\n",
    "print (train_seq_pad.shape, train_label.shape, train_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask =  tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def top_p_filter(l, thres=0.80):\n",
    "    sorted_idx = np.argsort(l).tolist()\n",
    "    idx = sorted_idx.pop()\n",
    "    psum = l[idx]\n",
    "    output = [idx]\n",
    "    while 1:\n",
    "        idx = sorted_idx.pop()\n",
    "        p = l[idx]\n",
    "        psum += p\n",
    "        if psum > thres:\n",
    "            break\n",
    "        output.append(idx)\n",
    "    return output\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads, d_model)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, look_ahead_mask,  training=True):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1 = self.mha(x, x, attention_mask=look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out = self.layernorm1(attn1 + x)\n",
    "\n",
    "        ffn_output = self.ffn(out)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out = self.layernorm2(ffn_output + out)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, maximum_position_encoding=50, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Dense(d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, look_ahead_mask, training=True):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, look_ahead_mask, training)\n",
    "        return x\n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocabulary_size, seq_len=50, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff)\n",
    "        self.mask = create_look_ahead_mask(seq_len+1)\n",
    "        self.final_layer = tf.keras.layers.Dense(vocabulary_size, activation='softmax')\n",
    "        \n",
    "    def call(self, inp, training):\n",
    "        \n",
    "        dec_output = self.decoder(inp, self.mask, training)\n",
    "        final_output = self.final_layer(dec_output)[:, :49] # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "d_model = 32\n",
    "num_heads = 8\n",
    "dff = 32\n",
    "\n",
    "tsfm = Transformer(num_layers, d_model, num_heads, dff, vocabulary_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
    "tsfm.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                           min_delta=0, \n",
    "                                                           patience=5,\n",
    "                                                           restore_best_weights=True)\n",
    "tsfm.fit(train_inp, \n",
    "         train_seq, \n",
    "         validation_split=0.1, \n",
    "         batch_size=256, \n",
    "         epochs=500,\n",
    "         callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.zeros((1, 50), dtype=int)\n",
    "c = np.ones_like(r) * 5\n",
    "r = np.eye(8)[r]\n",
    "c = np.eye(10)[c] \n",
    "r[:, 0, :8] = 0\n",
    "inp = np.concatenate([r, c], -1)\n",
    "\n",
    "for i in range(49):\n",
    "    out = tsfm.predict(inp)\n",
    "    out = out[0, i]\n",
    "    out = top_p_filter(out)\n",
    "    idx = np.random.choice(out)\n",
    "    inp[:, i+1, :8] = np.eye(8)[idx]\n",
    "    \n",
    "out = inp[0, 1:, :8]\n",
    "encoding_indices = np.argmax(out, -1)\n",
    "plt.imshow(encoding_indices.reshape(7, 7))\n",
    "plt.show()\n",
    "\n",
    "dec_inp = tf.gather(model.vq.embeddings, tf.cast(encoding_indices,tf.int64), axis=1)\n",
    "dec_inp = tf.transpose(dec_inp)\n",
    "out = model.dec(tf.reshape(dec_inp[tf.newaxis], (1, 7, 7, 128)))\n",
    "plt.imshow(out[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
